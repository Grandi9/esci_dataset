{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a2e173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\saiasg\\appdata\\local\\miniconda3\\envs\\esci\\lib\\site-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\saiasg\\appdata\\local\\miniconda3\\envs\\esci\\lib\\site-packages (from datasets) (2.3.4)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\saiasg\\appdata\\local\\miniconda3\\envs\\esci\\lib\\site-packages (from datasets) (22.0.0)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\saiasg\\appdata\\local\\miniconda3\\envs\\esci\\lib\\site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\saiasg\\appdata\\local\\miniconda3\\envs\\esci\\lib\\site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\saiasg\\appdata\\local\\miniconda3\\envs\\esci\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\saiasg\\appdata\\local\\miniconda3\\envs\\esci\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.6.0-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.9.0,>=2023.1.0 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in c:\\users\\saiasg\\appdata\\local\\miniconda3\\envs\\esci\\lib\\site-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\saiasg\\appdata\\local\\miniconda3\\envs\\esci\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\saiasg\\appdata\\local\\miniconda3\\envs\\esci\\lib\\site-packages (from datasets) (6.0.3)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.13.2-cp311-cp311-win_amd64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\saiasg\\appdata\\local\\miniconda3\\envs\\esci\\lib\\site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\saiasg\\appdata\\local\\miniconda3\\envs\\esci\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\saiasg\\appdata\\local\\miniconda3\\envs\\esci\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\saiasg\\appdata\\local\\miniconda3\\envs\\esci\\lib\\site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\saiasg\\appdata\\local\\miniconda3\\envs\\esci\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\saiasg\\appdata\\local\\miniconda3\\envs\\esci\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\saiasg\\appdata\\local\\miniconda3\\envs\\esci\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.8.0-cp311-cp311-win_amd64.whl.metadata (21 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.7.0-cp311-cp311-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.4.1-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.22.0-cp311-cp311-win_amd64.whl.metadata (77 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\saiasg\\appdata\\local\\miniconda3\\envs\\esci\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\saiasg\\appdata\\local\\miniconda3\\envs\\esci\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\saiasg\\appdata\\local\\miniconda3\\envs\\esci\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\saiasg\\appdata\\local\\miniconda3\\envs\\esci\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\saiasg\\appdata\\local\\miniconda3\\envs\\esci\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\saiasg\\appdata\\local\\miniconda3\\envs\\esci\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\saiasg\\appdata\\local\\miniconda3\\envs\\esci\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\saiasg\\appdata\\local\\miniconda3\\envs\\esci\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-4.3.0-py3-none-any.whl (506 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Using cached fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading aiohttp-3.13.2-cp311-cp311-win_amd64.whl (456 kB)\n",
      "Downloading multidict-6.7.0-cp311-cp311-win_amd64.whl (46 kB)\n",
      "Downloading yarl-1.22.0-cp311-cp311-win_amd64.whl (86 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.8.0-cp311-cp311-win_amd64.whl (44 kB)\n",
      "Downloading propcache-0.4.1-cp311-cp311-win_amd64.whl (41 kB)\n",
      "Downloading xxhash-3.6.0-cp311-cp311-win_amd64.whl (31 kB)\n",
      "Installing collected packages: xxhash, propcache, multidict, fsspec, frozenlist, dill, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "\n",
      "  Attempting uninstall: fsspec\n",
      "\n",
      "    Found existing installation: fsspec 2025.10.0\n",
      "\n",
      "    Uninstalling fsspec-2025.10.0:\n",
      "\n",
      "      Successfully uninstalled fsspec-2025.10.0\n",
      "\n",
      "   ---------- -----------------------------  3/12 [fsspec]\n",
      "   ---------- -----------------------------  3/12 [fsspec]\n",
      "   ------------- --------------------------  4/12 [frozenlist]\n",
      "   ---------------- -----------------------  5/12 [dill]\n",
      "   ----------------------- ----------------  7/12 [yarl]\n",
      "   -------------------------- -------------  8/12 [multiprocess]\n",
      "   --------------------------------- ------ 10/12 [aiohttp]\n",
      "   --------------------------------- ------ 10/12 [aiohttp]\n",
      "   --------------------------------- ------ 10/12 [aiohttp]\n",
      "   ------------------------------------ --- 11/12 [datasets]\n",
      "   ------------------------------------ --- 11/12 [datasets]\n",
      "   ------------------------------------ --- 11/12 [datasets]\n",
      "   ------------------------------------ --- 11/12 [datasets]\n",
      "   ---------------------------------------- 12/12 [datasets]\n",
      "\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 datasets-4.3.0 dill-0.4.0 frozenlist-1.8.0 fsspec-2025.9.0 multidict-6.7.0 multiprocess-0.70.16 propcache-0.4.1 xxhash-3.6.0 yarl-1.22.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/datasets/\n"
     ]
    }
   ],
   "source": [
    "#!pip install pyarrow\n",
    "#!pip install chromadb\n",
    "#!pip install sentence_transformers\n",
    "#!pip install tfidf_index\n",
    "#!pip install pandas\n",
    "#!pip install numpy\n",
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f20845d",
   "metadata": {},
   "source": [
    "# Step - 1 Load the data\n",
    "\n",
    "1a - Filtered for task-1, and 'us' and 'e' esci (exact, substitute, complement, and irrelevant)\n",
    "\n",
    "1b - Merge the Datasets using product_locale and product_id (connects each query/label pair to its corresponding product information)\n",
    "\n",
    "1c - Filtering the large merged DataFrame down to the required data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb00db75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "df_examples =pd.read_parquet(r'C:\\Users\\saiasg\\OneDrive - kochind.com\\Desktop\\Projects\\esci\\esci_dataset\\dataset\\shopping_queries_dataset_examples.parquet')\n",
    "df_products = pd.read_parquet(r'C:\\Users\\saiasg\\OneDrive - kochind.com\\Desktop\\Projects\\esci\\esci_dataset\\dataset\\shopping_queries_dataset_products.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a20757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(\n",
    "    df_examples,\n",
    "    df_products,\n",
    "    how='left',\n",
    "    left_on=['product_locale','product_id'],\n",
    "    right_on=['product_locale', 'product_id']\n",
    ")\n",
    "\n",
    "# filter for Task 1 (using the 'small_version' flag)\n",
    "df_task_1 = df_merged[df_merged[\"small_version\"] == 1]\n",
    "\n",
    "# filter for the training set\n",
    "df_task_1_train = df_task_1[df_task_1[\"split\"] == \"train\"]\n",
    "\n",
    "# applying filters for 'us' locale and 'E' label\n",
    "final_training_data = df_task_1_train[\n",
    "    (df_task_1_train[\"product_locale\"] == \"us\") &\n",
    "    (df_task_1_train[\"esci_label\"] == \"E\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "330d924d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_id</th>\n",
       "      <th>query</th>\n",
       "      <th>query_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_locale</th>\n",
       "      <th>esci_label</th>\n",
       "      <th>small_version</th>\n",
       "      <th>large_version</th>\n",
       "      <th>split</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_description</th>\n",
       "      <th>product_bullet_point</th>\n",
       "      <th>product_brand</th>\n",
       "      <th>product_color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>!awnmower tires without rims</td>\n",
       "      <td>1</td>\n",
       "      <td>B08L3B9B9P</td>\n",
       "      <td>us</td>\n",
       "      <td>E</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>MaxAuto 2-Pack 13x5.00-6 2PLY Turf Mower Tract...</td>\n",
       "      <td>MaxAuto 2-Pack 13x5.00-6 2PLY Turf Mower Tract...</td>\n",
       "      <td>Please check your existing tire Sidewall for t...</td>\n",
       "      <td>MaxAuto</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>!awnmower tires without rims</td>\n",
       "      <td>1</td>\n",
       "      <td>B07C1WZG12</td>\n",
       "      <td>us</td>\n",
       "      <td>E</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>(Set of 2) 15x6.00-6 Husqvarna/Poulan Tire Whe...</td>\n",
       "      <td>No fuss. Just take off your old assembly and r...</td>\n",
       "      <td>Tire size:15x6.00-6 Ply: 4 Tubeless\\n6x4.5 Whe...</td>\n",
       "      <td>Antego Tire &amp; Wheel</td>\n",
       "      <td>Husqvarna Silver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>!awnmower tires without rims</td>\n",
       "      <td>1</td>\n",
       "      <td>B077QMNXTS</td>\n",
       "      <td>us</td>\n",
       "      <td>E</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>MaxAuto 2 Pcs 16x6.50-8 Lawn Mower Tire for Ga...</td>\n",
       "      <td>&lt;br&gt;Tire Specifications:&lt;br&gt; 1. Material: Rubb...</td>\n",
       "      <td>Set of 2 16X6.50-8, 16x6.50x8, 16-6.50-8 Lawn ...</td>\n",
       "      <td>MaxAuto</td>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>!awnmower tires without rims</td>\n",
       "      <td>1</td>\n",
       "      <td>B06XX6BM2R</td>\n",
       "      <td>us</td>\n",
       "      <td>E</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>MARASTAR 21446-2PK 15x6.00-6\" Front Tire Assem...</td>\n",
       "      <td>None</td>\n",
       "      <td>Tire: 2 pack 15x6. 00-6 tube-type turf SAVER t...</td>\n",
       "      <td>MARASTAR</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>!awnmower tires without rims</td>\n",
       "      <td>1</td>\n",
       "      <td>B0089RNSNM</td>\n",
       "      <td>us</td>\n",
       "      <td>E</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>Honda 42710-VE2-M02ZE (Replaces 42710-VE2-M01Z...</td>\n",
       "      <td>Honda 42710-VE2-M02ZE (Replaces 42710-VE2-M01Z...</td>\n",
       "      <td>Set of 2 Honda OEM Rear Wheels\\nReplaces 42710...</td>\n",
       "      <td>Honda</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    example_id                         query  query_id  product_id  \\\n",
       "17          17  !awnmower tires without rims         1  B08L3B9B9P   \n",
       "20          20  !awnmower tires without rims         1  B07C1WZG12   \n",
       "21          21  !awnmower tires without rims         1  B077QMNXTS   \n",
       "23          23  !awnmower tires without rims         1  B06XX6BM2R   \n",
       "26          26  !awnmower tires without rims         1  B0089RNSNM   \n",
       "\n",
       "   product_locale esci_label  small_version  large_version  split  \\\n",
       "17             us          E              1              1  train   \n",
       "20             us          E              1              1  train   \n",
       "21             us          E              1              1  train   \n",
       "23             us          E              1              1  train   \n",
       "26             us          E              1              1  train   \n",
       "\n",
       "                                        product_title  \\\n",
       "17  MaxAuto 2-Pack 13x5.00-6 2PLY Turf Mower Tract...   \n",
       "20  (Set of 2) 15x6.00-6 Husqvarna/Poulan Tire Whe...   \n",
       "21  MaxAuto 2 Pcs 16x6.50-8 Lawn Mower Tire for Ga...   \n",
       "23  MARASTAR 21446-2PK 15x6.00-6\" Front Tire Assem...   \n",
       "26  Honda 42710-VE2-M02ZE (Replaces 42710-VE2-M01Z...   \n",
       "\n",
       "                                  product_description  \\\n",
       "17  MaxAuto 2-Pack 13x5.00-6 2PLY Turf Mower Tract...   \n",
       "20  No fuss. Just take off your old assembly and r...   \n",
       "21  <br>Tire Specifications:<br> 1. Material: Rubb...   \n",
       "23                                               None   \n",
       "26  Honda 42710-VE2-M02ZE (Replaces 42710-VE2-M01Z...   \n",
       "\n",
       "                                 product_bullet_point        product_brand  \\\n",
       "17  Please check your existing tire Sidewall for t...              MaxAuto   \n",
       "20  Tire size:15x6.00-6 Ply: 4 Tubeless\\n6x4.5 Whe...  Antego Tire & Wheel   \n",
       "21  Set of 2 16X6.50-8, 16x6.50x8, 16-6.50-8 Lawn ...              MaxAuto   \n",
       "23  Tire: 2 pack 15x6. 00-6 tube-type turf SAVER t...             MARASTAR   \n",
       "26  Set of 2 Honda OEM Rear Wheels\\nReplaces 42710...                Honda   \n",
       "\n",
       "       product_color  \n",
       "17              None  \n",
       "20  Husqvarna Silver  \n",
       "21             Black  \n",
       "23              None  \n",
       "26              None  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final_training_data\n",
    "final_training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0170f084",
   "metadata": {},
   "source": [
    "# Step - 2: Create sample dataset\n",
    "\n",
    "Creating a 500-row sample from the training data, centered on 50 unique queries.\n",
    "\n",
    "2a: Random sampling of rows\n",
    "\n",
    "2b: (Simple Random Sampling), select 50 unique queries and then sample row\n",
    "\n",
    "2c: Stratified sampling with around 60 unique queries to find 50 that yield a sample close to 500 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55dc8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Total available rows in master dataset: 181819\n"
     ]
    }
   ],
   "source": [
    "df_full = final_training_data.reset_index(drop=True)\n",
    "print(f\"1. Total rows in dataset: {len(df_full)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f429274c",
   "metadata": {},
   "source": [
    "#### 2a: Random sampling of rows - Basic barebones\n",
    "\n",
    "We just randomly grab 500 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6498d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Total available rows in master dataset: 181819\n",
      "\n",
      "--- Step 2a: Simple Random Sample ---\n",
      "We took a simple random sample of 500 rows from the full dataset.\n",
      "Result: 490 unique queries in the sample.\n",
      "\n",
      "Sample 2a Head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>product_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>drainer for sink</td>\n",
       "      <td>Outivity Sink Drain Shelf Triangular Sink Bask...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chernobyl</td>\n",
       "      <td>Azure Dust: Inside Chernobyl's Exclusion Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>retaine eye drops for dry eyes preservative free</td>\n",
       "      <td>UrsaPharm Hylo-Forte Lubricating Eye Drops 10M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>long sleeve pink dress</td>\n",
       "      <td>R.Vivimos Women's Autumn Winter Cotton Long Sl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(hearing aid not amplifer)</td>\n",
       "      <td>Hearing Aids, Enjoyee Hearing Aids for Seniors...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              query  \\\n",
       "0                                  drainer for sink   \n",
       "1                                         chernobyl   \n",
       "2  retaine eye drops for dry eyes preservative free   \n",
       "3                            long sleeve pink dress   \n",
       "4                        (hearing aid not amplifer)   \n",
       "\n",
       "                                       product_title  \n",
       "0  Outivity Sink Drain Shelf Triangular Sink Bask...  \n",
       "1      Azure Dust: Inside Chernobyl's Exclusion Zone  \n",
       "2  UrsaPharm Hylo-Forte Lubricating Eye Drops 10M...  \n",
       "3  R.Vivimos Women's Autumn Winter Cotton Long Sl...  \n",
       "4  Hearing Aids, Enjoyee Hearing Aids for Seniors...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "RANDOM_STATE = 42\n",
    "TARGET_QUERIES = 50\n",
    "TARGET_ROWS = 500\n",
    "\n",
    "df_full = final_training_data.reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n--- Step 2a: Simple Random Sample ---\")\n",
    "sample_2a = df_full.sample(n=TARGET_ROWS, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "final_unique_2a = sample_2a['query'].nunique()\n",
    "\n",
    "print(f\"Result: {final_unique_2a} unique queries in the sample.\")\n",
    "print(\"\\nSample Head:\")\n",
    "display(sample_2a[['query', 'product_title']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39183541",
   "metadata": {},
   "source": [
    "#### Step 2b: Simple Random Sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df69b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2b: Sample from Filtered Queries ---\n",
      "-> We selected 50 unique queries.\n",
      "-> Total associated rows available after filtering: 329\n",
      "Filtered set is too small (329 rows), using all available rows.\n",
      "Result: 329 rows and 50 unique queries in the sample.\n",
      "\n",
      "Sample 2b Head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>product_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6 dining chairs</td>\n",
       "      <td>Yaheetech Dining Chairs Velvet Armchairs for C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6 dining chairs</td>\n",
       "      <td>CozyCasa Dining Chairs Modern Style Dining Cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6 dining chairs</td>\n",
       "      <td>Yaheetech Dining Chairs with Waterproof leathe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6 dining chairs</td>\n",
       "      <td>Yaheetech Dining Chairs Dining Room Chairs Liv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6 dining chairs</td>\n",
       "      <td>Modern Dining Chairs Set of 6 - Faux Leather D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             query                                      product_title\n",
       "0  6 dining chairs  Yaheetech Dining Chairs Velvet Armchairs for C...\n",
       "1  6 dining chairs  CozyCasa Dining Chairs Modern Style Dining Cha...\n",
       "2  6 dining chairs  Yaheetech Dining Chairs with Waterproof leathe...\n",
       "3  6 dining chairs  Yaheetech Dining Chairs Dining Room Chairs Liv...\n",
       "4  6 dining chairs  Modern Dining Chairs Set of 6 - Faux Leather D..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"\\n--- Step 2b: Sample from Filtered Queries ---\")\n",
    "\n",
    "# 1. randomly select 50 unique queries\n",
    "unique_queries_2b = df_full['query'].unique()\n",
    "if len(unique_queries_2b) < TARGET_QUERIES:\n",
    "    sample_queries_2b = pd.Series(unique_queries_2b)\n",
    "else:\n",
    "    sample_queries_2b = pd.Series(unique_queries_2b).sample(n=TARGET_QUERIES, random_state=RANDOM_STATE)\n",
    "\n",
    "# 2. filter master set to products matching these queries\n",
    "df_filtered_2b = df_full[df_full['query'].isin(sample_queries_2b)].reset_index(drop=True)\n",
    "total_rows_for_queries_2b = len(df_filtered_2b)\n",
    "\n",
    "print(f\"-> selected {len(sample_queries_2b)} unique queries.\")\n",
    "print(f\"-> total rows available after filtering: {total_rows_for_queries_2b}\")\n",
    "\n",
    "# 3. sample 500 rows from filtered dataset\n",
    "if total_rows_for_queries_2b <= TARGET_ROWS:\n",
    "    sample_2b = df_filtered_2b.copy()\n",
    "    print(f\"filtered set too small ({total_rows_for_queries_2b} rows), using all available.\")\n",
    "else:\n",
    "    sample_2b = df_filtered_2b.sample(n=TARGET_ROWS, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "    print(f\"took random sample of {len(sample_2b)} rows from filtered dataset.\")\n",
    "\n",
    "# 4. final results\n",
    "final_unique_2b = sample_2b['query'].nunique()\n",
    "print(f\"result: {len(sample_2b)} rows and {final_unique_2b} unique queries in sample.\")\n",
    "\n",
    "print(\"\\nSample 2b Head:\")\n",
    "display(sample_2b[['query', 'product_title']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c785e03e",
   "metadata": {},
   "source": [
    "#### 2c Stratified sampling\n",
    "\n",
    "Using this approach to select queries by row count to maximize sample diversity\n",
    "\n",
    "To ensures representation from all queries, and also high-frequency queries, using this approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b6ba90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2c: Stratified Sampling for Target Rows and Queries ---\n",
      "Randomly selected 70 potential unique queries.\n",
      "Filtered down to 520 rows associated with these potential queries.\n",
      "\n",
      "Row counts for potential queries:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "query\n",
       "turning shoe                         30\n",
       "6 dining chairs                      22\n",
       "plants                               22\n",
       "tan and brown bathroom wall decor    21\n",
       "poleras deportivas mujer             19\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected 50 queries that yield a total of 485 rows.\n",
      "Final filtered set has 485 rows, less than the target 500. Using all available rows.\n",
      "\n",
      "Final Sample (2c) Result: 485 rows and 50 unique queries.\n",
      "\n",
      "Final Sample (2c) Head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>product_title</th>\n",
       "      <th>esci_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6 dining chairs</td>\n",
       "      <td>Yaheetech Dining Chairs Velvet Armchairs for C...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6 dining chairs</td>\n",
       "      <td>CozyCasa Dining Chairs Modern Style Dining Cha...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6 dining chairs</td>\n",
       "      <td>Yaheetech Dining Chairs with Waterproof leathe...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6 dining chairs</td>\n",
       "      <td>Yaheetech Dining Chairs Dining Room Chairs Liv...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6 dining chairs</td>\n",
       "      <td>Modern Dining Chairs Set of 6 - Faux Leather D...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             query                                      product_title  \\\n",
       "0  6 dining chairs  Yaheetech Dining Chairs Velvet Armchairs for C...   \n",
       "1  6 dining chairs  CozyCasa Dining Chairs Modern Style Dining Cha...   \n",
       "2  6 dining chairs  Yaheetech Dining Chairs with Waterproof leathe...   \n",
       "3  6 dining chairs  Yaheetech Dining Chairs Dining Room Chairs Liv...   \n",
       "4  6 dining chairs  Modern Dining Chairs Set of 6 - Faux Leather D...   \n",
       "\n",
       "  esci_label  \n",
       "0          E  \n",
       "1          E  \n",
       "2          E  \n",
       "3          E  \n",
       "4          E  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"\\n--- Step 2c: Stratified Sampling for Target Rows and Queries ---\")\n",
    "\n",
    "TARGET_QUERIES_2c = 70 # aiming for higher number of unique queries initially\n",
    "\n",
    "# 1. randomly selecting 70 unique queries\n",
    "unique_queries_full = df_full['query'].unique()\n",
    "if len(unique_queries_full) < TARGET_QUERIES_2c:\n",
    "    potential_sample_queries = pd.Series(unique_queries_full)\n",
    "    print(f\"only {len(unique_queries_full)} unique queries available, using all.\")\n",
    "else:\n",
    "    potential_sample_queries = pd.Series(unique_queries_full).sample(n=TARGET_QUERIES_2c, random_state=RANDOM_STATE)\n",
    "    print(f\"randomly selected {len(potential_sample_queries)} potential unique queries.\")\n",
    "\n",
    "# 2. filtering rows associated with selected queries\n",
    "df_potential_queries = df_full[df_full['query'].isin(potential_sample_queries)].reset_index(drop=True)\n",
    "print(f\"filtered down to {len(df_potential_queries)} rows associated with these queries.\")\n",
    "\n",
    "# 3. calculate rows per query\n",
    "query_row_counts = df_potential_queries.groupby('query').size().sort_values(ascending=False)\n",
    "print(\"\\nRow counts for potential queries:\")\n",
    "display(query_row_counts.head())\n",
    "\n",
    "# 4. select top 50 queries by row count\n",
    "selected_queries_2c = []\n",
    "current_row_count = 0\n",
    "for query, count in query_row_counts.items():\n",
    "    if len(selected_queries_2c) < 50:\n",
    "         selected_queries_2c.append(query)\n",
    "         current_row_count += count\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# ensure we have exactly 50 queries if available\n",
    "if len(selected_queries_2c) < 50 and len(potential_sample_queries) >= 50:\n",
    "     print(f\"warning: could only select {len(selected_queries_2c)} queries.\")\n",
    "elif len(selected_queries_2c) == 50:\n",
    "     print(f\"\\nselected 50 queries that yield {current_row_count} rows.\")\n",
    "\n",
    "# 5. filter to final 50 queries\n",
    "df_final_50_queries = df_potential_queries[df_potential_queries['query'].isin(selected_queries_2c)].reset_index(drop=True)\n",
    "\n",
    "# 6. determine final sample based on row count\n",
    "if len(df_final_50_queries) == TARGET_ROWS:\n",
    "    sample_2c = df_final_50_queries.copy()\n",
    "    print(f\"final filtered set has exactly {TARGET_ROWS} rows.\")\n",
    "elif len(df_final_50_queries) < TARGET_ROWS:\n",
    "    sample_2c = df_final_50_queries.copy()\n",
    "    print(f\"final filtered set has {len(df_final_50_queries)} rows, using all available.\")\n",
    "else:\n",
    "    sample_2c = df_final_50_queries.sample(n=TARGET_ROWS, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "    print(f\"final filtered set has {len(df_final_50_queries)} rows, taking random sample of {TARGET_ROWS}.\")\n",
    "\n",
    "# 7. final results\n",
    "final_unique_2c = sample_2c['query'].nunique()\n",
    "print(f\"\\nfinal sample (2c) result: {len(sample_2c)} rows and {final_unique_2c} unique queries.\")\n",
    "\n",
    "print(\"\\nFinal Sample (2c) Head:\")\n",
    "display(sample_2c[['query', 'product_title', 'esci_label']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5101af08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the sample_2c DataFrame to a CSV file\n",
    "sample_2c.to_csv(r'C:\\Users\\saiasg\\OneDrive - kochind.com\\Desktop\\Projects\\esci\\esci_dataset\\sample_2c_full_data.csv', index=False)\n",
    "\n",
    "sample_2c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fadfa4d",
   "metadata": {},
   "source": [
    "### Some Data Explroing\n",
    "\n",
    "Intresting queires: One query was  \"usb2aub2ra1m\"\n",
    "\n",
    "And its apprently a product id for right anlged usb connector\n",
    "\n",
    "https://www.startech.com/en-eu/cables/usb2aub2ra1m?srsltid=AfmBOoryvB93OxhVQnPUAocknMNz41MVDvr2TJMrWf0ijRnCwf5htlXn\n",
    "\n",
    "Face urine? - Fake urine but still never knew these existed haha\n",
    "\n",
    "And some plumbing related queires: zurn qkipsp 5 port plastic manifold without valves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829b6289",
   "metadata": {},
   "source": [
    "# Step 3: Vector Index\n",
    "\n",
    "3a - Baseline tf-idf \n",
    "\n",
    "3b - Dense model (all-MiniLM-L6-v2)\n",
    "\n",
    "3c - Two tower dense model\n",
    "\n",
    "3d - Hybrid model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ca474a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saiasg\\AppData\\Local\\miniconda3\\envs\\esci\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "import chromadb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import CrossEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bf6adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_id</th>\n",
       "      <th>query</th>\n",
       "      <th>query_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_locale</th>\n",
       "      <th>esci_label</th>\n",
       "      <th>small_version</th>\n",
       "      <th>large_version</th>\n",
       "      <th>split</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_description</th>\n",
       "      <th>product_bullet_point</th>\n",
       "      <th>product_brand</th>\n",
       "      <th>product_color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>118060</td>\n",
       "      <td>6 dining chairs</td>\n",
       "      <td>4845</td>\n",
       "      <td>B08CZ6TC2L</td>\n",
       "      <td>us</td>\n",
       "      <td>E</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>Yaheetech Dining Chairs Velvet Armchairs for C...</td>\n",
       "      <td>Set of 6 Kitchen Dining Chairs for Counter Lou...</td>\n",
       "      <td>STRONG METAL LEGS: To enhance the weight capac...</td>\n",
       "      <td>Yaheetech</td>\n",
       "      <td>Grey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118064</td>\n",
       "      <td>6 dining chairs</td>\n",
       "      <td>4845</td>\n",
       "      <td>B08HQG1MFS</td>\n",
       "      <td>us</td>\n",
       "      <td>E</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>CozyCasa Dining Chairs Modern Style Dining Cha...</td>\n",
       "      <td>&lt;b&gt;If you are in search of some quality-reliab...</td>\n",
       "      <td>Dining Chairs set of 6 -- White PP backrest an...</td>\n",
       "      <td>CozyCasa</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>118065</td>\n",
       "      <td>6 dining chairs</td>\n",
       "      <td>4845</td>\n",
       "      <td>B08K2K3J4C</td>\n",
       "      <td>us</td>\n",
       "      <td>E</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>Yaheetech Dining Chairs with Waterproof leathe...</td>\n",
       "      <td>Make every long-time sitting comfortable. The ...</td>\n",
       "      <td>MULTIPLE USE: Sold in a set of 6 chairs. Desig...</td>\n",
       "      <td>Yaheetech</td>\n",
       "      <td>Brown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>118066</td>\n",
       "      <td>6 dining chairs</td>\n",
       "      <td>4845</td>\n",
       "      <td>B08K2V66N8</td>\n",
       "      <td>us</td>\n",
       "      <td>E</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>Yaheetech Dining Chairs Dining Room Chairs Liv...</td>\n",
       "      <td>Make every dinner time comfortable. Constructe...</td>\n",
       "      <td>MULTIPLE USE: Sold in a set of 6 chairs. This ...</td>\n",
       "      <td>Yaheetech</td>\n",
       "      <td>Khaki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>118067</td>\n",
       "      <td>6 dining chairs</td>\n",
       "      <td>4845</td>\n",
       "      <td>B08K8VDTW8</td>\n",
       "      <td>us</td>\n",
       "      <td>E</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>Modern Dining Chairs Set of 6 - Faux Leather D...</td>\n",
       "      <td>&lt;b&gt;Modern Dining Chairs Set of 6 - Faux Leathe...</td>\n",
       "      <td>Comfortable Dining Chairs Set of 6 - The dinin...</td>\n",
       "      <td>WENYU</td>\n",
       "      <td>Grey</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   example_id            query  query_id  product_id product_locale  \\\n",
       "0      118060  6 dining chairs      4845  B08CZ6TC2L             us   \n",
       "1      118064  6 dining chairs      4845  B08HQG1MFS             us   \n",
       "2      118065  6 dining chairs      4845  B08K2K3J4C             us   \n",
       "3      118066  6 dining chairs      4845  B08K2V66N8             us   \n",
       "4      118067  6 dining chairs      4845  B08K8VDTW8             us   \n",
       "\n",
       "  esci_label  small_version  large_version  split  \\\n",
       "0          E              1              1  train   \n",
       "1          E              1              1  train   \n",
       "2          E              1              1  train   \n",
       "3          E              1              1  train   \n",
       "4          E              1              1  train   \n",
       "\n",
       "                                       product_title  \\\n",
       "0  Yaheetech Dining Chairs Velvet Armchairs for C...   \n",
       "1  CozyCasa Dining Chairs Modern Style Dining Cha...   \n",
       "2  Yaheetech Dining Chairs with Waterproof leathe...   \n",
       "3  Yaheetech Dining Chairs Dining Room Chairs Liv...   \n",
       "4  Modern Dining Chairs Set of 6 - Faux Leather D...   \n",
       "\n",
       "                                 product_description  \\\n",
       "0  Set of 6 Kitchen Dining Chairs for Counter Lou...   \n",
       "1  <b>If you are in search of some quality-reliab...   \n",
       "2  Make every long-time sitting comfortable. The ...   \n",
       "3  Make every dinner time comfortable. Constructe...   \n",
       "4  <b>Modern Dining Chairs Set of 6 - Faux Leathe...   \n",
       "\n",
       "                                product_bullet_point product_brand  \\\n",
       "0  STRONG METAL LEGS: To enhance the weight capac...     Yaheetech   \n",
       "1  Dining Chairs set of 6 -- White PP backrest an...      CozyCasa   \n",
       "2  MULTIPLE USE: Sold in a set of 6 chairs. Desig...     Yaheetech   \n",
       "3  MULTIPLE USE: Sold in a set of 6 chairs. This ...     Yaheetech   \n",
       "4  Comfortable Dining Chairs Set of 6 - The dinin...         WENYU   \n",
       "\n",
       "  product_color  \n",
       "0          Grey  \n",
       "1         White  \n",
       "2         Brown  \n",
       "3         Khaki  \n",
       "4          Grey  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the stored dataset\n",
    "df_sample_2c = pd.read_csv(r'C:\\Users\\saiasg\\OneDrive - kochind.com\\Desktop\\Projects\\esci\\esci_dataset\\sample_2c_full_data.csv')\n",
    "\n",
    "df_sample_2c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde9408f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data into a unique Product Corpus...\n",
      "Created a corpus of 485 unique products.\n",
      "Created an evaluation set of 485 query-product pairs.\n"
     ]
    }
   ],
   "source": [
    "# create product corpus (de-duplicated products)\n",
    "print(\"Processing data into a unique Product Corpus...\")\n",
    "product_columns = [\n",
    "    'product_id', \n",
    "    'product_title', \n",
    "    'product_description', \n",
    "    'product_bullet_point', \n",
    "    'product_brand', \n",
    "    'product_color'\n",
    "]\n",
    "product_corpus_df = df_sample_2c[product_columns].drop_duplicates(subset=['product_id']).reset_index(drop=True)\n",
    "\n",
    "# fill nans with empty strings\n",
    "text_cols_to_fill = product_columns[1:]\n",
    "for col in text_cols_to_fill:\n",
    "    product_corpus_df[col] = product_corpus_df[col].fillna('')\n",
    "\n",
    "# combine all text fields into single product_text for embedding\n",
    "product_corpus_df['product_text'] = (\n",
    "    product_corpus_df['product_title'] + ' ' +\n",
    "    product_corpus_df['product_brand'] + ' ' +\n",
    "    product_corpus_df['product_color'] + ' ' +\n",
    "    product_corpus_df['product_description'] + ' ' +\n",
    "    product_corpus_df['product_bullet_point']\n",
    ")\n",
    "# cleaning up extra whitespaces\n",
    "product_corpus_df['product_text'] = product_corpus_df['product_text'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "print(f\"created corpus of {len(product_corpus_df)} unique products.\")\n",
    "\n",
    "# creating query evaluation set (query-to-product pairs)\n",
    "query_eval_set = df_sample_2c[['query', 'query_id', 'product_id', 'esci_label']].copy()\n",
    "print(f\"created evaluation set of {len(query_eval_set)} query-product pairs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae17ca7",
   "metadata": {},
   "source": [
    "## 3a: tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "600a6370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TF-IDF embeddings (sparse vectors) for 485 documents...\n",
      "TF-IDF matrix created with shape: (485, 5000)\n",
      "Building in-memory sparse index with scikit-learn (NearestNeighbors)...\n",
      "In-memory sparse index built successfully.\n"
     ]
    }
   ],
   "source": [
    "documents = product_corpus_df['product_text'].tolist()\n",
    "\n",
    "print(f\"Creating TF-IDF embeddings (sparse vectors) for {len(documents)} documents...\")\n",
    "\n",
    "# stop words and limit to the top 5000 most frequent terms\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "\n",
    "# Create the sparse TF-IDF matrix\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "print(f\"TF-IDF matrix created with shape: {tfidf_matrix.shape}\")\n",
    "\n",
    "# Using 'cosine' similarity and 'brute' force\n",
    "print(\"Building in-memory sparse index with scikit-learn (NearestNeighbors)...\")\n",
    "n_neighbors = 10\n",
    "nn_index = NearestNeighbors(n_neighbors=n_neighbors, metric='cosine', algorithm='brute')\n",
    "nn_index.fit(tfidf_matrix)\n",
    "print(\"In-memory sparse index built successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c88708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Query: 'carpenter bench press'\n",
      "Ground Truth 'Exact' Product IDs (8): ['B088R8VC7V', 'B07N4QN64D', 'B01M4F8JZJ', 'B01LZV1QW1', 'B00HQONFVE', 'B00BHSPJC8', 'B00068U7XQ', 'B00SIQ1DLS']\n",
      "\n",
      "Searching index for Top 10 results...\n"
     ]
    }
   ],
   "source": [
    "test_query = \"carpenter bench press\" # Not using a direct numeric or direct word serach\n",
    "\n",
    "# Find the ground truth for this query from our in-memory DataFrame\n",
    "ground_truth_df = query_eval_set[query_eval_set['query'] == test_query]\n",
    "ground_truth_ids = ground_truth_df['product_id'].tolist()\n",
    "\n",
    "print(f\"Test Query: '{test_query}'\")\n",
    "print(f\"Ground Truth 'Exact' Product IDs ({len(ground_truth_ids)}): {ground_truth_ids}\")\n",
    "\n",
    "# Embed the test query *using the same vectorizer*\n",
    "# .transform returns a sparse 2D matrix\n",
    "query_vector = tfidf_vectorizer.transform([test_query])\n",
    "\n",
    "# Search the index\n",
    "print(f\"\\nSearching index for Top {n_neighbors} results...\")\n",
    "# .kneighbors returns (distances, indices)\n",
    "distances, indices = nn_index.kneighbors(query_vector)\n",
    "\n",
    "# Flatten the results from 2D to 1D\n",
    "result_indices = indices[0]\n",
    "result_distances = distances[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccd16ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Results (lower distance is better):\n",
      "     product_id  _distance  is_ground_truth  \\\n",
      "73   B01LZV1QW1   0.813588             True   \n",
      "70   B088R8VC7V   0.868601             True   \n",
      "76   B00068U7XQ   0.883066             True   \n",
      "431  B06XD1DVBG   0.885864            False   \n",
      "71   B07N4QN64D   0.893176             True   \n",
      "427  B06XD5G2XB   0.911316            False   \n",
      "77   B00SIQ1DLS   0.912220             True   \n",
      "74   B00HQONFVE   0.916837             True   \n",
      "129  B07G15QZNQ   0.924956            False   \n",
      "342  B0015TX0MU   0.925376            False   \n",
      "\n",
      "                                         product_title  \n",
      "73   Genesis GDP805P 5-Speed 2.6 Amp 8\" Drill Press...  \n",
      "70   Workbench Mounted Drilling Machine, 350W 5 Spe...  \n",
      "76              Palmgren Ratcheting arbor press, 3 ton  \n",
      "431  Creative Teaching Press Safari Friends Calenda...  \n",
      "71   Drill Stand for Hand Drill, Electric Bench Cla...  \n",
      "427  CTP Woodland Friends Calendar Set Bulletin Boa...  \n",
      "77   Yost M7WW Rapid Acting Wood Working Vise, 7\", ...  \n",
      "74   WEN 4208T 2.3-Amp 8-Inch 5-Speed Benchtop Dril...  \n",
      "129  Diving Flashlight 18000 Lumen IPX8 Scuba Dive ...  \n",
      "342  Office Star Table And Chair Set, Light Grey, B...  \n",
      "\n",
      "Query-specific Metric: Found 6 out of 8 ground truth items in the Top 10 results.\n"
     ]
    }
   ],
   "source": [
    "# Get the full product info from the original corpus_df using the indices\n",
    "results_df = product_corpus_df.iloc[result_indices].copy()\n",
    "\n",
    "# Add the distance and a ground_truth check\n",
    "results_df['_distance'] = result_distances\n",
    "results_df['is_ground_truth'] = results_df['product_id'].isin(ground_truth_ids)\n",
    "\n",
    "print(results_df[['product_id', '_distance', 'is_ground_truth', 'product_title']])\n",
    "matches_in_top_10 = results_df['is_ground_truth'].sum()\n",
    "print(f\"\\nFound {matches_in_top_10} out of {len(ground_truth_ids)} ground truth items in the Top 10 results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df319cc",
   "metadata": {},
   "source": [
    "## 3b: Dense model (all-MiniLM-L6-v2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4d6030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "# Setup, Constants, and Model Loading\n",
    "MODEL_NAME = 'all-MiniLM-L6-v2'\n",
    "DB_PATH = \"./chroma_data\"\n",
    "COLLECTION_NAME = \"product_embeddings\"\n",
    "\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bd48d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 16/16 [00:06<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generated.\n",
      "Creating ChromaDB collection 'product_embeddings' at './chroma_data'...\n",
      "Adding 485 vectors to collection...\n",
      "Successfully created collection with 485 vectors.\n"
     ]
    }
   ],
   "source": [
    "# Embed Documents for Indexing\n",
    "documents = product_corpus_df['product_text'].tolist()\n",
    "product_ids = product_corpus_df['product_id'].tolist()\n",
    "\n",
    "print(f\"Embedding {len(documents)}\")\n",
    "embeddings = model.encode(documents, show_progress_bar=True)\n",
    "print(\"Embeddings generated.\")\n",
    "\n",
    "# Prepare data for ChromaDB\n",
    "str_product_ids = [str(pid) for pid in product_ids]\n",
    "metadatas = [{\"product_id\": pid, \"text\": doc} for pid, doc in zip(product_ids, documents)]\n",
    "\n",
    "# Creating the ChromaDB Persistent Index\n",
    "if os.path.exists(DB_PATH):\n",
    "    shutil.rmtree(DB_PATH)\n",
    "client = chromadb.PersistentClient(path=DB_PATH)\n",
    "\n",
    "print(f\"Creating ChromaDB collection '{COLLECTION_NAME}' at '{DB_PATH}'...\")\n",
    "collection = client.get_or_create_collection(\n",
    "    name=COLLECTION_NAME, \n",
    "    metadata={\"hnsw:space\": \"l2\"} # 'l2' (Euclidean) distance\n",
    ")\n",
    "\n",
    "# adding the data\n",
    "collection.add(\n",
    "    embeddings=embeddings.tolist(),\n",
    "    documents=documents,\n",
    "    metadatas=metadatas,\n",
    "    ids=str_product_ids # Chroma requires a list of unique string IDs\n",
    ")\n",
    "print(f\"Successfully created collection with {collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc55e203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Query: 'carpenter bench press'\n",
      "Ground Truth 'Exact' Product IDs (8): ['B088R8VC7V', 'B07N4QN64D', 'B01M4F8JZJ', 'B01LZV1QW1', 'B00HQONFVE', 'B00BHSPJC8', 'B00068U7XQ', 'B00SIQ1DLS']\n",
      "\n",
      "Searching index for Top 10 results...\n",
      "Search Results (lower distance is better):\n",
      "   product_id  _distance  is_ground_truth  \\\n",
      "0  B07N4QN64D   0.949831             True   \n",
      "1  B088R8VC7V   1.055872             True   \n",
      "2  B00HQONFVE   1.058271             True   \n",
      "3  B00068U7XQ   1.136232             True   \n",
      "4  B01LZV1QW1   1.194515             True   \n",
      "5  B00SIQ1DLS   1.298030             True   \n",
      "6  B07HH3K4SK   1.401191            False   \n",
      "7  B01GOM6OUM   1.409457            False   \n",
      "8  B00BHSPJC8   1.445602             True   \n",
      "9  B078WZLFHG   1.452312            False   \n",
      "\n",
      "                                       product_title  \n",
      "0  Drill Stand for Hand Drill, Electric Bench Cla...  \n",
      "1  Workbench Mounted Drilling Machine, 350W 5 Spe...  \n",
      "2  WEN 4208T 2.3-Amp 8-Inch 5-Speed Benchtop Dril...  \n",
      "3             Palmgren Ratcheting arbor press, 3 ton  \n",
      "4  Genesis GDP805P 5-Speed 2.6 Amp 8\" Drill Press...  \n",
      "5  Yost M7WW Rapid Acting Wood Working Vise, 7\", ...  \n",
      "6  Giantex Set of 6 Modern Dining Chairs w/Plasti...  \n",
      "7  Inspirer Studio Set of 6 New 17 inch SeatDepth...  \n",
      "8      Shop Fox D4328 9-Inch Quick Release Wood Vise  \n",
      "9  Signature Design by Ashley Porter Dining Room ...  \n",
      "\n",
      "Query-specific Metric: Found 7 out of 8 ground truth items in the Top 10 results.\n",
      "\n",
      "--- Compare this to our TF-IDF result ---\n",
      "TF-IDF Metric: Found 6 out of 8 ground truth items in the Top 10 results.\n"
     ]
    }
   ],
   "source": [
    "test_query = \"carpenter bench press\"\n",
    "\n",
    "# ground truth for this query from our in-memory DataFrame\n",
    "ground_truth_df = query_eval_set[query_eval_set['query'] == test_query]\n",
    "ground_truth_ids = ground_truth_df['product_id'].tolist()\n",
    "print(f\"Test Query: '{test_query}'\")\n",
    "print(f\"Ground Truth 'Exact' Product IDs ({len(ground_truth_ids)}): {ground_truth_ids}\")\n",
    "\n",
    "# Embed and search\n",
    "query_vector = model.encode([test_query]).tolist()\n",
    "n_neighbors = 10\n",
    "print(f\"\\nSearching index for Top {n_neighbors} results...\")\n",
    "search_results = collection.query(\n",
    "    query_embeddings=query_vector,\n",
    "    n_results=n_neighbors,\n",
    ")\n",
    "\n",
    "print(\"Search Results (lower distance is better):\")\n",
    "\n",
    "# Chroma's output format\n",
    "result_metadatas = search_results['metadatas'][0]\n",
    "result_distances = search_results['distances'][0]\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'product_id': [meta['product_id'] for meta in result_metadatas],\n",
    "    '_distance': result_distances\n",
    "})\n",
    "\n",
    "# Add a column to show if the result is a \"Ground Truth\" match\n",
    "results_df['is_ground_truth'] = results_df['product_id'].isin(ground_truth_ids)\n",
    "\n",
    "\n",
    "results_df = results_df.merge(product_corpus_df[['product_id', 'product_title']], on='product_id', how='left')\n",
    "print(results_df[['product_id', '_distance', 'is_ground_truth', 'product_title']])\n",
    "\n",
    "# metric for this query\n",
    "matches_in_top_10 = results_df['is_ground_truth'].sum()\n",
    "print(f\"\\nQuery-specific Metric: Found {matches_in_top_10} out of {len(ground_truth_ids)} ground truth items in the Top 10 results.\")\n",
    "\n",
    "print(\"\\n--- Compare this to our TF-IDF Metric: Found 6 out of 8 ground truth items in the Top 10 results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0cee19",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cabd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ground truth mapping...\n",
      "Found 50 unique queries to evaluate.\n",
      "\n",
      "Running TF-IDF evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TF-IDF Evaluation: 100%|| 50/50 [00:00<00:00, 786.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Dense model evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dense Model Evaluation: 100%|| 50/50 [00:00<00:00, 80.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation completed!\n",
      "\n",
      "TF-IDF Average Metrics:\n",
      "HITS@1: 0.130\n",
      "HITS@5: 0.503\n",
      "HITS@10: 0.689\n",
      "MRR: 0.284\n",
      "\n",
      "Dense Model Average Metrics:\n",
      "HITS@1: 0.137\n",
      "HITS@5: 0.554\n",
      "HITS@10: 0.748\n",
      "MRR: 0.308\n",
      "\n",
      "Per-Query Results:\n",
      "\n",
      "Query: 6 dining chairs\n",
      "TF-IDF: {'hits': {'hits@1': 0.045454545454545456, 'hits@5': 0.22727272727272727, 'hits@10': 0.45454545454545453}, 'mrr': 0.13313492063492063, 'query': '6 dining chairs'}\n",
      "Dense: {'hits': {'hits@1': 0.045454545454545456, 'hits@5': 0.22727272727272727, 'hits@10': 0.4090909090909091}, 'mrr': 0.12555916305916306, 'query': '6 dining chairs'}\n",
      "\n",
      "Query: a intex pool pump\n",
      "TF-IDF: {'hits': {'hits@1': 0.16666666666666666, 'hits@5': 0.8333333333333334, 'hits@10': 1.0}, 'mrr': 0.40833333333333327, 'query': 'a intex pool pump'}\n",
      "Dense: {'hits': {'hits@1': 0.16666666666666666, 'hits@5': 0.8333333333333334, 'hits@10': 1.0}, 'mrr': 0.4083333333333334, 'query': 'a intex pool pump'}\n",
      "\n",
      "Query: activated carbon mask\n",
      "TF-IDF: {'hits': {'hits@1': 0.1, 'hits@5': 0.5, 'hits@10': 0.8}, 'mrr': 0.2717857142857143, 'query': 'activated carbon mask'}\n",
      "Dense: {'hits': {'hits@1': 0.1, 'hits@5': 0.5, 'hits@10': 0.9}, 'mrr': 0.2828968253968254, 'query': 'activated carbon mask'}\n",
      "\n",
      "Query: adidas original superstar women\n",
      "TF-IDF: {'hits': {'hits@1': 0.16666666666666666, 'hits@5': 0.8333333333333334, 'hits@10': 1.0}, 'mrr': 0.4083333333333334, 'query': 'adidas original superstar women'}\n",
      "Dense: {'hits': {'hits@1': 0.16666666666666666, 'hits@5': 0.8333333333333334, 'hits@10': 1.0}, 'mrr': 0.4083333333333334, 'query': 'adidas original superstar women'}\n",
      "\n",
      "Query: balloons yellow and orange\n",
      "TF-IDF: {'hits': {'hits@1': 0.058823529411764705, 'hits@5': 0.29411764705882354, 'hits@10': 0.5882352941176471}, 'mrr': 0.1722922502334267, 'query': 'balloons yellow and orange'}\n",
      "Dense: {'hits': {'hits@1': 0.058823529411764705, 'hits@5': 0.29411764705882354, 'hits@10': 0.5882352941176471}, 'mrr': 0.17229225023342673, 'query': 'balloons yellow and orange'}\n",
      "\n",
      "Query: big little lies\n",
      "TF-IDF: {'hits': {'hits@1': 0.16666666666666666, 'hits@5': 0.5, 'hits@10': 0.5}, 'mrr': 0.3055555555555555, 'query': 'big little lies'}\n",
      "Dense: {'hits': {'hits@1': 0.16666666666666666, 'hits@5': 0.6666666666666666, 'hits@10': 0.6666666666666666}, 'mrr': 0.33888888888888885, 'query': 'big little lies'}\n",
      "\n",
      "Query: brumate hopsulator slim\n",
      "TF-IDF: {'hits': {'hits@1': 0.3333333333333333, 'hits@5': 1.0, 'hits@10': 1.0}, 'mrr': 0.611111111111111, 'query': 'brumate hopsulator slim'}\n",
      "Dense: {'hits': {'hits@1': 0.3333333333333333, 'hits@5': 1.0, 'hits@10': 1.0}, 'mrr': 0.611111111111111, 'query': 'brumate hopsulator slim'}\n",
      "\n",
      "Query: carpenter bench press\n",
      "TF-IDF: {'hits': {'hits@1': 0.125, 'hits@5': 0.5, 'hits@10': 0.75}, 'mrr': 0.2876488095238095, 'query': 'carpenter bench press'}\n",
      "Dense: {'hits': {'hits@1': 0.125, 'hits@5': 0.625, 'hits@10': 0.875}, 'mrr': 0.32013888888888886, 'query': 'carpenter bench press'}\n",
      "\n",
      "Query: christmas dresses size 3t-4t\n",
      "TF-IDF: {'hits': {'hits@1': 0.07692307692307693, 'hits@5': 0.38461538461538464, 'hits@10': 0.7692307692307693}, 'mrr': 0.22530525030525034, 'query': 'christmas dresses size 3t-4t'}\n",
      "Dense: {'hits': {'hits@1': 0.07692307692307693, 'hits@5': 0.38461538461538464, 'hits@10': 0.7692307692307693}, 'mrr': 0.22530525030525034, 'query': 'christmas dresses size 3t-4t'}\n",
      "\n",
      "Query: computers\n",
      "TF-IDF: {'hits': {'hits@1': 0.06666666666666667, 'hits@5': 0.13333333333333333, 'hits@10': 0.13333333333333333}, 'mrr': 0.1, 'query': 'computers'}\n",
      "Dense: {'hits': {'hits@1': 0.06666666666666667, 'hits@5': 0.3333333333333333, 'hits@10': 0.5333333333333333}, 'mrr': 0.17462962962962963, 'query': 'computers'}\n",
      "\n",
      "Query: cow utter balm\n",
      "TF-IDF: {'hits': {'hits@1': 0.09090909090909091, 'hits@5': 0.45454545454545453, 'hits@10': 0.9090909090909091}, 'mrr': 0.26626984126984127, 'query': 'cow utter balm'}\n",
      "Dense: {'hits': {'hits@1': 0.09090909090909091, 'hits@5': 0.45454545454545453, 'hits@10': 0.9090909090909091}, 'mrr': 0.26626984126984127, 'query': 'cow utter balm'}\n",
      "\n",
      "Query: daisy buck\n",
      "TF-IDF: {'hits': {'hits@1': 0.14285714285714285, 'hits@5': 0.7142857142857143, 'hits@10': 1.0}, 'mrr': 0.37040816326530607, 'query': 'daisy buck'}\n",
      "Dense: {'hits': {'hits@1': 0.14285714285714285, 'hits@5': 0.7142857142857143, 'hits@10': 1.0}, 'mrr': 0.3658730158730159, 'query': 'daisy buck'}\n",
      "\n",
      "Query: dive light\n",
      "TF-IDF: {'hits': {'hits@1': 0.07692307692307693, 'hits@5': 0.3076923076923077, 'hits@10': 0.6153846153846154}, 'mrr': 0.2003052503052503, 'query': 'dive light'}\n",
      "Dense: {'hits': {'hits@1': 0.07692307692307693, 'hits@5': 0.38461538461538464, 'hits@10': 0.7692307692307693}, 'mrr': 0.22530525030525034, 'query': 'dive light'}\n",
      "\n",
      "Query: drama book\n",
      "TF-IDF: {'hits': {'hits@1': 0.16666666666666666, 'hits@5': 0.6666666666666666, 'hits@10': 0.8333333333333334}, 'mrr': 0.3138888888888889, 'query': 'drama book'}\n",
      "Dense: {'hits': {'hits@1': 0.16666666666666666, 'hits@5': 0.6666666666666666, 'hits@10': 0.8333333333333334}, 'mrr': 0.3666666666666667, 'query': 'drama book'}\n",
      "\n",
      "Query: ethique perfector\n",
      "TF-IDF: {'hits': {'hits@1': 0.09090909090909091, 'hits@5': 0.45454545454545453, 'hits@10': 0.9090909090909091}, 'mrr': 0.26626984126984127, 'query': 'ethique perfector'}\n",
      "Dense: {'hits': {'hits@1': 0.09090909090909091, 'hits@5': 0.18181818181818182, 'hits@10': 0.6363636363636364}, 'mrr': 0.1950577200577201, 'query': 'ethique perfector'}\n",
      "\n",
      "Query: face urine for drug test\n",
      "TF-IDF: {'hits': {'hits@1': 0.1111111111111111, 'hits@5': 0.5555555555555556, 'hits@10': 0.6666666666666666}, 'mrr': 0.27222222222222225, 'query': 'face urine for drug test'}\n",
      "Dense: {'hits': {'hits@1': 0.1111111111111111, 'hits@5': 0.5555555555555556, 'hits@10': 0.8888888888888888}, 'mrr': 0.29845679012345677, 'query': 'face urine for drug test'}\n",
      "\n",
      "Query: fathes\n",
      "TF-IDF: {'hits': {'hits@1': 0.0, 'hits@5': 0.0, 'hits@10': 0.0}, 'mrr': 0.0, 'query': 'fathes'}\n",
      "Dense: {'hits': {'hits@1': 0.0, 'hits@5': 0.0, 'hits@10': 0.0}, 'mrr': 0.0, 'query': 'fathes'}\n",
      "\n",
      "Query: fiberglass sealer\n",
      "TF-IDF: {'hits': {'hits@1': 0.1, 'hits@5': 0.5, 'hits@10': 0.9}, 'mrr': 0.2828968253968254, 'query': 'fiberglass sealer'}\n",
      "Dense: {'hits': {'hits@1': 0.1, 'hits@5': 0.5, 'hits@10': 0.9}, 'mrr': 0.2817857142857143, 'query': 'fiberglass sealer'}\n",
      "\n",
      "Query: gaming chair with footrest and speakers\n",
      "TF-IDF: {'hits': {'hits@1': 0.25, 'hits@5': 1.0, 'hits@10': 1.0}, 'mrr': 0.5208333333333333, 'query': 'gaming chair with footrest and speakers'}\n",
      "Dense: {'hits': {'hits@1': 0.25, 'hits@5': 1.0, 'hits@10': 1.0}, 'mrr': 0.5208333333333333, 'query': 'gaming chair with footrest and speakers'}\n",
      "\n",
      "Query: incase macbook air 13 inch case\n",
      "TF-IDF: {'hits': {'hits@1': 0.08333333333333333, 'hits@5': 0.4166666666666667, 'hits@10': 0.8333333333333334}, 'mrr': 0.24408068783068784, 'query': 'incase macbook air 13 inch case'}\n",
      "Dense: {'hits': {'hits@1': 0.08333333333333333, 'hits@5': 0.4166666666666667, 'hits@10': 0.5833333333333334}, 'mrr': 0.21458333333333332, 'query': 'incase macbook air 13 inch case'}\n",
      "\n",
      "Query: jack skellington beanie\n",
      "TF-IDF: {'hits': {'hits@1': 0.16666666666666666, 'hits@5': 0.8333333333333334, 'hits@10': 1.0}, 'mrr': 0.4083333333333334, 'query': 'jack skellington beanie'}\n",
      "Dense: {'hits': {'hits@1': 0.16666666666666666, 'hits@5': 0.6666666666666666, 'hits@10': 0.8333333333333334}, 'mrr': 0.3680555555555555, 'query': 'jack skellington beanie'}\n",
      "\n",
      "Query: jeans for men slim fit 29x28\n",
      "TF-IDF: {'hits': {'hits@1': 0.1111111111111111, 'hits@5': 0.5555555555555556, 'hits@10': 1.0}, 'mrr': 0.311552028218695, 'query': 'jeans for men slim fit 29x28'}\n",
      "Dense: {'hits': {'hits@1': 0.1111111111111111, 'hits@5': 0.5555555555555556, 'hits@10': 0.8888888888888888}, 'mrr': 0.30198412698412697, 'query': 'jeans for men slim fit 29x28'}\n",
      "\n",
      "Query: kay correll sweet river series\n",
      "TF-IDF: {'hits': {'hits@1': 0.14285714285714285, 'hits@5': 0.7142857142857143, 'hits@10': 1.0}, 'mrr': 0.3704081632653061, 'query': 'kay correll sweet river series'}\n",
      "Dense: {'hits': {'hits@1': 0.14285714285714285, 'hits@5': 0.7142857142857143, 'hits@10': 0.8571428571428571}, 'mrr': 0.35, 'query': 'kay correll sweet river series'}\n",
      "\n",
      "Query: logitech g933\n",
      "TF-IDF: {'hits': {'hits@1': 0.25, 'hits@5': 1.0, 'hits@10': 1.0}, 'mrr': 0.5208333333333333, 'query': 'logitech g933'}\n",
      "Dense: {'hits': {'hits@1': 0.25, 'hits@5': 1.0, 'hits@10': 1.0}, 'mrr': 0.5208333333333333, 'query': 'logitech g933'}\n",
      "\n",
      "Query: long sweater\n",
      "TF-IDF: {'hits': {'hits@1': 0.08333333333333333, 'hits@5': 0.3333333333333333, 'hits@10': 0.75}, 'mrr': 0.20241402116402119, 'query': 'long sweater'}\n",
      "Dense: {'hits': {'hits@1': 0.08333333333333333, 'hits@5': 0.4166666666666667, 'hits@10': 0.6666666666666666}, 'mrr': 0.22384259259259257, 'query': 'long sweater'}\n",
      "\n",
      "Query: mac book pro 13 cover\n",
      "TF-IDF: {'hits': {'hits@1': 0.0, 'hits@5': 0.3333333333333333, 'hits@10': 0.8333333333333334}, 'mrr': 0.1673280423280423, 'query': 'mac book pro 13 cover'}\n",
      "Dense: {'hits': {'hits@1': 0.16666666666666666, 'hits@5': 0.6666666666666666, 'hits@10': 0.6666666666666666}, 'mrr': 0.34722222222222215, 'query': 'mac book pro 13 cover'}\n",
      "\n",
      "Query: nathan vaporair 7l 2.0\n",
      "TF-IDF: {'hits': {'hits@1': 0.25, 'hits@5': 1.0, 'hits@10': 1.0}, 'mrr': 0.5208333333333333, 'query': 'nathan vaporair 7l 2.0'}\n",
      "Dense: {'hits': {'hits@1': 0.25, 'hits@5': 1.0, 'hits@10': 1.0}, 'mrr': 0.5208333333333334, 'query': 'nathan vaporair 7l 2.0'}\n",
      "\n",
      "Query: pa4 micrpphone prime\n",
      "TF-IDF: {'hits': {'hits@1': 0.0, 'hits@5': 0.0, 'hits@10': 0.0}, 'mrr': 0.0, 'query': 'pa4 micrpphone prime'}\n",
      "Dense: {'hits': {'hits@1': 0.125, 'hits@5': 0.5, 'hits@10': 0.75}, 'mrr': 0.29910714285714285, 'query': 'pa4 micrpphone prime'}\n",
      "\n",
      "Query: pinhole glasses\n",
      "TF-IDF: {'hits': {'hits@1': 0.16666666666666666, 'hits@5': 0.6666666666666666, 'hits@10': 0.8333333333333334}, 'mrr': 0.375, 'query': 'pinhole glasses'}\n",
      "Dense: {'hits': {'hits@1': 0.16666666666666666, 'hits@5': 0.8333333333333334, 'hits@10': 0.8333333333333334}, 'mrr': 0.38055555555555554, 'query': 'pinhole glasses'}\n",
      "\n",
      "Query: plants\n",
      "TF-IDF: {'hits': {'hits@1': 0.045454545454545456, 'hits@5': 0.22727272727272727, 'hits@10': 0.45454545454545453}, 'mrr': 0.13313492063492063, 'query': 'plants'}\n",
      "Dense: {'hits': {'hits@1': 0.045454545454545456, 'hits@5': 0.22727272727272727, 'hits@10': 0.45454545454545453}, 'mrr': 0.13313492063492063, 'query': 'plants'}\n",
      "\n",
      "Query: poleras deportivas mujer\n",
      "TF-IDF: {'hits': {'hits@1': 0.0, 'hits@5': 0.0, 'hits@10': 0.0}, 'mrr': 0.0, 'query': 'poleras deportivas mujer'}\n",
      "Dense: {'hits': {'hits@1': 0.0, 'hits@5': 0.0, 'hits@10': 0.05263157894736842}, 'mrr': 0.005263157894736842, 'query': 'poleras deportivas mujer'}\n",
      "\n",
      "Query: rainbow brite costume women\n",
      "TF-IDF: {'hits': {'hits@1': 0.1, 'hits@5': 0.5, 'hits@10': 0.9}, 'mrr': 0.28289682539682537, 'query': 'rainbow brite costume women'}\n",
      "Dense: {'hits': {'hits@1': 0.1, 'hits@5': 0.5, 'hits@10': 0.9}, 'mrr': 0.28039682539682537, 'query': 'rainbow brite costume women'}\n",
      "\n",
      "Query: red bows for crafts\n",
      "TF-IDF: {'hits': {'hits@1': 0.07692307692307693, 'hits@5': 0.38461538461538464, 'hits@10': 0.7692307692307693}, 'mrr': 0.22530525030525028, 'query': 'red bows for crafts'}\n",
      "Dense: {'hits': {'hits@1': 0.07692307692307693, 'hits@5': 0.38461538461538464, 'hits@10': 0.7692307692307693}, 'mrr': 0.22530525030525034, 'query': 'red bows for crafts'}\n",
      "\n",
      "Query: saucony jazz mens olive\n",
      "TF-IDF: {'hits': {'hits@1': 0.25, 'hits@5': 1.0, 'hits@10': 1.0}, 'mrr': 0.5208333333333333, 'query': 'saucony jazz mens olive'}\n",
      "Dense: {'hits': {'hits@1': 0.25, 'hits@5': 1.0, 'hits@10': 1.0}, 'mrr': 0.5208333333333333, 'query': 'saucony jazz mens olive'}\n",
      "\n",
      "Query: spotify with alexa\n",
      "TF-IDF: {'hits': {'hits@1': 0.3333333333333333, 'hits@5': 0.6666666666666666, 'hits@10': 0.6666666666666666}, 'mrr': 0.5, 'query': 'spotify with alexa'}\n",
      "Dense: {'hits': {'hits@1': 0.3333333333333333, 'hits@5': 1.0, 'hits@10': 1.0}, 'mrr': 0.5833333333333334, 'query': 'spotify with alexa'}\n",
      "\n",
      "Query: square table with chairs\n",
      "TF-IDF: {'hits': {'hits@1': 0.125, 'hits@5': 0.25, 'hits@10': 0.625}, 'mrr': 0.24181547619047616, 'query': 'square table with chairs'}\n",
      "Dense: {'hits': {'hits@1': 0.0, 'hits@5': 0.375, 'hits@10': 0.5}, 'mrr': 0.14791666666666667, 'query': 'square table with chairs'}\n",
      "\n",
      "Query: stephenie meyer the seeker\n",
      "TF-IDF: {'hits': {'hits@1': 0.25, 'hits@5': 0.5, 'hits@10': 0.5}, 'mrr': 0.375, 'query': 'stephenie meyer the seeker'}\n",
      "Dense: {'hits': {'hits@1': 0.25, 'hits@5': 0.5, 'hits@10': 0.75}, 'mrr': 0.4166666666666667, 'query': 'stephenie meyer the seeker'}\n",
      "\n",
      "Query: tan and brown bathroom wall decor\n",
      "TF-IDF: {'hits': {'hits@1': 0.047619047619047616, 'hits@5': 0.23809523809523808, 'hits@10': 0.47619047619047616}, 'mrr': 0.13947467876039304, 'query': 'tan and brown bathroom wall decor'}\n",
      "Dense: {'hits': {'hits@1': 0.047619047619047616, 'hits@5': 0.23809523809523808, 'hits@10': 0.47619047619047616}, 'mrr': 0.13947467876039304, 'query': 'tan and brown bathroom wall decor'}\n",
      "\n",
      "Query: the chalice and the blade by riane eisler\n",
      "TF-IDF: {'hits': {'hits@1': 0.16666666666666666, 'hits@5': 0.8333333333333334, 'hits@10': 0.8333333333333334}, 'mrr': 0.38055555555555554, 'query': 'the chalice and the blade by riane eisler'}\n",
      "Dense: {'hits': {'hits@1': 0.16666666666666666, 'hits@5': 0.6666666666666666, 'hits@10': 0.8333333333333334}, 'mrr': 0.375, 'query': 'the chalice and the blade by riane eisler'}\n",
      "\n",
      "Query: the theory that would not die\n",
      "TF-IDF: {'hits': {'hits@1': 0.25, 'hits@5': 0.5, 'hits@10': 0.5}, 'mrr': 0.3333333333333333, 'query': 'the theory that would not die'}\n",
      "Dense: {'hits': {'hits@1': 0.25, 'hits@5': 0.5, 'hits@10': 0.5}, 'mrr': 0.3, 'query': 'the theory that would not die'}\n",
      "\n",
      "Query: turning shoe\n",
      "TF-IDF: {'hits': {'hits@1': 0.03333333333333333, 'hits@5': 0.16666666666666666, 'hits@10': 0.3333333333333333}, 'mrr': 0.09763227513227514, 'query': 'turning shoe'}\n",
      "Dense: {'hits': {'hits@1': 0.03333333333333333, 'hits@5': 0.16666666666666666, 'hits@10': 0.3333333333333333}, 'mrr': 0.09763227513227514, 'query': 'turning shoe'}\n",
      "\n",
      "Query: usb2aub2ra1m\n",
      "TF-IDF: {'hits': {'hits@1': 0.0, 'hits@5': 0.0, 'hits@10': 0.0}, 'mrr': 0.0, 'query': 'usb2aub2ra1m'}\n",
      "Dense: {'hits': {'hits@1': 0.16666666666666666, 'hits@5': 0.8333333333333334, 'hits@10': 1.0}, 'mrr': 0.4083333333333334, 'query': 'usb2aub2ra1m'}\n",
      "\n",
      "Query: vanity table without mirror with drawers\n",
      "TF-IDF: {'hits': {'hits@1': 0.1111111111111111, 'hits@5': 0.5555555555555556, 'hits@10': 1.0}, 'mrr': 0.31432980599647264, 'query': 'vanity table without mirror with drawers'}\n",
      "Dense: {'hits': {'hits@1': 0.1111111111111111, 'hits@5': 0.5555555555555556, 'hits@10': 1.0}, 'mrr': 0.3143298059964727, 'query': 'vanity table without mirror with drawers'}\n",
      "\n",
      "Query: vibration sensor connect to phone\n",
      "TF-IDF: {'hits': {'hits@1': 0.3333333333333333, 'hits@5': 1.0, 'hits@10': 1.0}, 'mrr': 0.5833333333333334, 'query': 'vibration sensor connect to phone'}\n",
      "Dense: {'hits': {'hits@1': 0.3333333333333333, 'hits@5': 1.0, 'hits@10': 1.0}, 'mrr': 0.611111111111111, 'query': 'vibration sensor connect to phone'}\n",
      "\n",
      "Query: weather theme cutout\n",
      "TF-IDF: {'hits': {'hits@1': 0.08333333333333333, 'hits@5': 0.3333333333333333, 'hits@10': 0.4166666666666667}, 'mrr': 0.1798611111111111, 'query': 'weather theme cutout'}\n",
      "Dense: {'hits': {'hits@1': 0.08333333333333333, 'hits@5': 0.4166666666666667, 'hits@10': 0.5833333333333334}, 'mrr': 0.20902777777777778, 'query': 'weather theme cutout'}\n",
      "\n",
      "Query: winter ski half mask\n",
      "TF-IDF: {'hits': {'hits@1': 0.07692307692307693, 'hits@5': 0.23076923076923078, 'hits@10': 0.5384615384615384}, 'mrr': 0.1708180708180708, 'query': 'winter ski half mask'}\n",
      "Dense: {'hits': {'hits@1': 0.07692307692307693, 'hits@5': 0.38461538461538464, 'hits@10': 0.7692307692307693}, 'mrr': 0.22530525030525034, 'query': 'winter ski half mask'}\n",
      "\n",
      "Query: womens sport sweatpants\n",
      "TF-IDF: {'hits': {'hits@1': 0.2, 'hits@5': 0.6, 'hits@10': 0.8}, 'mrr': 0.35, 'query': 'womens sport sweatpants'}\n",
      "Dense: {'hits': {'hits@1': 0.2, 'hits@5': 0.6, 'hits@10': 0.8}, 'mrr': 0.37, 'query': 'womens sport sweatpants'}\n",
      "\n",
      "Query: xtc the big express\n",
      "TF-IDF: {'hits': {'hits@1': 0.125, 'hits@5': 0.375, 'hits@10': 0.375}, 'mrr': 0.22916666666666666, 'query': 'xtc the big express'}\n",
      "Dense: {'hits': {'hits@1': 0.125, 'hits@5': 0.375, 'hits@10': 0.375}, 'mrr': 0.22916666666666666, 'query': 'xtc the big express'}\n",
      "\n",
      "Query: yeti tall thin can cooler\n",
      "TF-IDF: {'hits': {'hits@1': 0.0625, 'hits@5': 0.3125, 'hits@10': 0.4375}, 'mrr': 0.16205357142857144, 'query': 'yeti tall thin can cooler'}\n",
      "Dense: {'hits': {'hits@1': 0.0625, 'hits@5': 0.3125, 'hits@10': 0.5625}, 'mrr': 0.1726438492063492, 'query': 'yeti tall thin can cooler'}\n",
      "\n",
      "Query: zurn qkipsp 5 port plastic manifold without valves\n",
      "TF-IDF: {'hits': {'hits@1': 0.25, 'hits@5': 0.75, 'hits@10': 0.75}, 'mrr': 0.4583333333333333, 'query': 'zurn qkipsp 5 port plastic manifold without valves'}\n",
      "Dense: {'hits': {'hits@1': 0.25, 'hits@5': 0.75, 'hits@10': 1.0}, 'mrr': 0.5, 'query': 'zurn qkipsp 5 port plastic manifold without valves'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics(ground_truth_ids, retrieved_ids, k_values=[1, 5, 10]):\n",
    "    \"\"\"Calculate recall@k and MRR for a query\"\"\"\n",
    "    metrics = {'hits': {}, 'mrr': 0.0}\n",
    "    \n",
    "    # hits@k for each k\n",
    "    for k in k_values:\n",
    "        top_k = retrieved_ids[:k]\n",
    "        hits = len(set(ground_truth_ids) & set(top_k)) / len(ground_truth_ids)\n",
    "        metrics['hits'][f'hits@{k}'] = hits\n",
    "    \n",
    "    # mean reciprocal rank\n",
    "    mrr = 0.0\n",
    "    for gt_id in ground_truth_ids:\n",
    "        if gt_id in retrieved_ids:\n",
    "            rank = retrieved_ids.index(gt_id) + 1\n",
    "            mrr += 1.0 / rank\n",
    "    \n",
    "    metrics['mrr'] = mrr / len(ground_truth_ids) if ground_truth_ids else 0.0\n",
    "    return metrics\n",
    "\n",
    "def evaluate_tfidf_model(query, ground_truth_ids, tfidf_vectorizer, nn_index, product_corpus_df, k=10):\n",
    "    \"\"\"Run evaluation for TF-IDF model\"\"\"\n",
    "    query_vector = tfidf_vectorizer.transform([query])\n",
    "    distances, indices = nn_index.kneighbors(query_vector)\n",
    "    result_indices = indices[0]\n",
    "    \n",
    "    retrieved_ids = product_corpus_df.iloc[result_indices]['product_id'].tolist()\n",
    "    metrics = calculate_metrics(ground_truth_ids, retrieved_ids)\n",
    "    metrics['query'] = query\n",
    "    return metrics\n",
    "\n",
    "def evaluate_dense_model(query, ground_truth_ids, model, collection, product_corpus_df, k=10):\n",
    "    \"\"\"Run evaluation for dense embedding model\"\"\"\n",
    "    query_vector = model.encode([query]).tolist()\n",
    "    \n",
    "    search_results = collection.query(\n",
    "        query_embeddings=query_vector,\n",
    "        n_results=k,\n",
    "    )\n",
    "    \n",
    "    retrieved_ids = [meta['product_id'] for meta in search_results['metadatas'][0]]\n",
    "    metrics = calculate_metrics(ground_truth_ids, retrieved_ids)\n",
    "    metrics['query'] = query\n",
    "    return metrics\n",
    "\n",
    "def print_avg_metrics(results, model_name):\n",
    "    \"\"\"Print average metrics across all queries\"\"\"\n",
    "    avg_hits = {k: np.mean([r['hits'][k] for r in results]) for k in results[0]['hits'].keys()}\n",
    "    avg_mrr = np.mean([r['mrr'] for r in results])\n",
    "    \n",
    "    print(f\"\\n{model_name} Average Metrics:\")\n",
    "    for k, v in avg_hits.items():\n",
    "        print(f\"{k.upper()}: {v:.3f}\")\n",
    "    print(f\"MRR: {avg_mrr:.3f}\")\n",
    "    \n",
    "    return avg_hits, avg_mrr\n",
    "\n",
    "# build ground truth mapping\n",
    "print(\"Creating ground truth mapping...\")\n",
    "ground_truth_map = {}\n",
    "for _, row in query_eval_set.iterrows():\n",
    "    query = row['query']\n",
    "    product_id = row['product_id']\n",
    "    \n",
    "    if query not in ground_truth_map:\n",
    "        ground_truth_map[query] = []\n",
    "    ground_truth_map[query].append(product_id)\n",
    "\n",
    "unique_queries = list(ground_truth_map.keys())\n",
    "print(f\"Found {len(unique_queries)} unique queries to evaluate.\")\n",
    "\n",
    "# run evaluations\n",
    "print(\"\\nRunning TF-IDF evaluation...\")\n",
    "tfidf_results = []\n",
    "for query in tqdm(unique_queries, desc=\"TF-IDF Evaluation\"):\n",
    "    ground_truth_ids = ground_truth_map[query]\n",
    "    metrics = evaluate_tfidf_model(query, ground_truth_ids, tfidf_vectorizer, nn_index, product_corpus_df)\n",
    "    tfidf_results.append(metrics)\n",
    "\n",
    "print(\"\\nRunning Dense model evaluation...\")\n",
    "dense_results = []\n",
    "for query in tqdm(unique_queries, desc=\"Dense Model Evaluation\"):\n",
    "    ground_truth_ids = ground_truth_map[query]\n",
    "    metrics = evaluate_dense_model(query, ground_truth_ids, model, collection, product_corpus_df)\n",
    "    dense_results.append(metrics)\n",
    "\n",
    "print(\"\\nEvaluation completed!\")\n",
    "\n",
    "# get average metrics\n",
    "tfidf_hits, tfidf_mrr = print_avg_metrics(tfidf_results, \"TF-IDF\")\n",
    "dense_hits, dense_mrr = print_avg_metrics(dense_results, \"Dense Model\")\n",
    "\n",
    "# extract individual metrics for analysis\n",
    "tfidf_mrrs = [r['mrr'] for r in tfidf_results]\n",
    "tfidf_recalls_at_1 = [r['hits']['hits@1'] for r in tfidf_results]\n",
    "tfidf_recalls_at_5 = [r['hits']['hits@5'] for r in tfidf_results]\n",
    "tfidf_recalls_at_10 = [r['hits']['hits@10'] for r in tfidf_results]\n",
    "\n",
    "dense_mrrs = [r['mrr'] for r in dense_results]\n",
    "dense_recalls_at_1 = [r['hits']['hits@1'] for r in dense_results]\n",
    "dense_recalls_at_5 = [r['hits']['hits@5'] for r in dense_results]\n",
    "dense_recalls_at_10 = [r['hits']['hits@10'] for r in dense_results]\n",
    "\n",
    "print(\"\\nPer-Query Results:\")\n",
    "for i, query in enumerate(unique_queries):\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"TF-IDF:\", tfidf_results[i])\n",
    "    print(\"Dense:\", dense_results[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a516e81",
   "metadata": {},
   "source": [
    "## Re-ranking\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3178453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Iteration 2: Re-ranking with a Cross-Encoder ---\n",
      "Defining evaluation metrics...\n",
      "Metric functions (MRR, HITS@k) defined.\n",
      "\n",
      "--- Part 2: Loading Models and Clients ---\n",
      "Loading re-ranking model: cross-encoder/ms-marco-MiniLM-L-6-v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saiasg\\AppData\\Local\\miniconda3\\envs\\esci\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\saiasg\\.cache\\huggingface\\hub\\models--cross-encoder--ms-marco-MiniLM-L-6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-ranker model loaded.\n",
      "Loading retrieval model: all-MiniLM-L6-v2...\n",
      "Retrieval model loaded.\n",
      "\n",
      "--- Part 3: Running Full Evaluation (Retrieve + Re-rank) ---\n",
      "Creating ground truth map...\n",
      "Found 50 unique queries to evaluate.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating (Retrieve + Re-rank): 100%|| 50/50 [01:50<00:00,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-ranking complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Defining evaluation metrics...\")\n",
    "\n",
    "def calculate_reciprocal_rank(retrieved_ids, ground_truth_ids):\n",
    "    \"\"\"Get reciprocal rank for a query\"\"\"\n",
    "    ground_truth_set = set(ground_truth_ids)\n",
    "    for i, p_id in enumerate(retrieved_ids):\n",
    "        if p_id in ground_truth_set:\n",
    "            return 1.0 / (i + 1)\n",
    "    return 0.0\n",
    "\n",
    "def calculate_recall_at_k(retrieved_ids, ground_truth_ids, k):\n",
    "    \"\"\"Get recall@k for a query\"\"\"\n",
    "    ground_truth_set = set(ground_truth_ids)\n",
    "    retrieved_at_k = set(retrieved_ids[:k])\n",
    "    \n",
    "    hits = len(ground_truth_set.intersection(retrieved_at_k))\n",
    "    return hits / len(ground_truth_set) if ground_truth_set else 0.0\n",
    "\n",
    "print(\"Metric functions defined.\")\n",
    "\n",
    "# load models\n",
    "print(\"\\n--- Loading Models ---\")\n",
    "\n",
    "RE_RANKER_MODEL = 'cross-encoder/ms-marco-MiniLM-L-6-v2'\n",
    "print(f\"Loading re-ranking model: {RE_RANKER_MODEL}...\")\n",
    "cross_encoder_model = CrossEncoder(RE_RANKER_MODEL)\n",
    "print(\"Re-ranker loaded.\")\n",
    "\n",
    "print(f\"Loading retrieval model: {MODEL_NAME}...\")\n",
    "retrieval_model = SentenceTransformer(MODEL_NAME)\n",
    "print(\"Retrieval model loaded.\")\n",
    "\n",
    "# run full evaluation (retrieve + re-rank)\n",
    "print(\"\\n--- Running Full Evaluation ---\")\n",
    "\n",
    "N_RETRIEVE = 50  # get more candidates for re-ranking\n",
    "N_NEIGHBORS_FINAL = 10  # final top-k for evaluation\n",
    "\n",
    "# build ground truth mapping\n",
    "print(\"Creating ground truth map...\")\n",
    "ground_truth_map = query_eval_set.groupby('query')['product_id'].apply(list).to_dict()\n",
    "unique_queries = list(ground_truth_map.keys())\n",
    "print(f\"Found {len(unique_queries)} unique queries to evaluate.\")\n",
    "\n",
    "# store results\n",
    "rerank_mrrs = []\n",
    "rerank_recalls_at_1 = []\n",
    "rerank_recalls_at_5 = []\n",
    "rerank_recalls_at_10 = []\n",
    "\n",
    "# evaluate each query\n",
    "for query in tqdm(unique_queries, desc=\"Evaluating (Retrieve + Re-rank)\"):\n",
    "    ground_truth_ids = ground_truth_map[query]\n",
    "    \n",
    "    # stage 1: retrieve candidates\n",
    "    query_vector_dense = retrieval_model.encode([query]).tolist()\n",
    "    \n",
    "    search_results = collection.query( \n",
    "        query_embeddings=query_vector_dense,\n",
    "        n_results=N_RETRIEVE,\n",
    "    )\n",
    "    \n",
    "    # extract candidates\n",
    "    retrieved_metadatas = search_results['metadatas'][0]\n",
    "    candidate_ids = [meta['product_id'] for meta in retrieved_metadatas]\n",
    "    candidate_texts = [meta['text'] for meta in retrieved_metadatas]\n",
    "\n",
    "    # stage 2: re-rank with cross-encoder\n",
    "    rerank_pairs = [(query, doc_text) for doc_text in candidate_texts]\n",
    "    rerank_scores = cross_encoder_model.predict(rerank_pairs)\n",
    "    \n",
    "    # sort by new scores\n",
    "    reranked_results = list(zip(candidate_ids, rerank_scores))\n",
    "    reranked_results.sort(key=lambda x: x[1], reverse=True)\n",
    "    retrieved_ids_reranked = [p_id for p_id, score in reranked_results]\n",
    "\n",
    "    # calculate metrics on re-ranked results\n",
    "    rerank_mrrs.append(calculate_reciprocal_rank(retrieved_ids_reranked, ground_truth_ids))\n",
    "    rerank_recalls_at_1.append(calculate_recall_at_k(retrieved_ids_reranked, ground_truth_ids, k=1))\n",
    "    rerank_recalls_at_5.append(calculate_recall_at_k(retrieved_ids_reranked, ground_truth_ids, k=5))\n",
    "    rerank_recalls_at_10.append(calculate_recall_at_k(retrieved_ids_reranked, ground_truth_ids, k=N_NEIGHBORS_FINAL))\n",
    "\n",
    "print(\"Re-ranking evaluation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87b11a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TF-IDF (Baseline) ---\n",
      "  Mean Reciprocal Rank (MRR): 0.2843\n",
      "  HITS@10 (Recall@10):        0.6891\n",
      "\n",
      "--- Dense Model (Baseline) ---\n",
      "  Mean Reciprocal Rank (MRR): 0.3076\n",
      "  HITS@10 (Recall@10):        0.7483\n",
      "\n",
      "--- Re-ranked Dense Model (Iteration 2) ---\n",
      "  Mean Reciprocal Rank (MRR): 0.9519\n",
      "  HITS@1 (Recall@1):          0.1381\n",
      "  HITS@5 (Recall@5):          0.5641\n",
      "  HITS@10 (Recall@10):        0.7858\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- TF-IDF (Baseline) ---\")\n",
    "print(f\"  Mean Reciprocal Rank (MRR): {np.mean(tfidf_mrrs):.4f}\") \n",
    "print(f\"  HITS@10 (Recall@10):        {np.mean(tfidf_recalls_at_10):.4f}\")\n",
    "\n",
    "print(\"\\n--- Dense Model (Baseline) ---\")\n",
    "print(f\"  Mean Reciprocal Rank (MRR): {np.mean(dense_mrrs):.4f}\") \n",
    "print(f\"  HITS@10 (Recall@10):        {np.mean(dense_recalls_at_10):.4f}\")\n",
    "\n",
    "print(\"\\n--- Re-ranked Dense Model (Iteration 2) ---\")\n",
    "print(f\"  Mean Reciprocal Rank (MRR): {np.mean(rerank_mrrs):.4f}\")\n",
    "print(f\"  HITS@1 (Recall@1):          {np.mean(rerank_recalls_at_1):.4f}\")\n",
    "print(f\"  HITS@5 (Recall@5):          {np.mean(rerank_recalls_at_5):.4f}\")\n",
    "print(f\"  HITS@10 (Recall@10):        {np.mean(rerank_recalls_at_10):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9189d21",
   "metadata": {},
   "source": [
    "# Step 4 Two tower eval vs others\n",
    "\n",
    "Refrence links:\n",
    "\n",
    "Uber blog on two tower arch: https://www.uber.com/blog/innovative-recommendation-applications-using-two-tower-embeddings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f9a8258a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Two-Tower Model Evaluation ===\n",
      " Two-Tower model found. Starting evaluation...\n",
      "Preparing evaluation data...\n",
      "Product corpus: 485 unique products\n",
      "Evaluation data: 485 query-product pairs\n",
      "Unique queries: 50 queries\n",
      "\n",
      " Building product embeddings index...\n",
      "Building product embeddings index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding products: 100%|| 16/16 [00:29<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product index built: (485, 256)\n",
      "\n",
      " Testing query: 'carpenter bench press'\n",
      " Ground Truth: 8 relevant products\n",
      "    Product IDs: ['B07N4QN64D', 'B088R8VC7V', 'B01M4F8JZJ', 'B00068U7XQ', 'B01LZV1QW1', 'B00HQONFVE', 'B00SIQ1DLS', 'B00BHSPJC8']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding querys: 100%|| 1/1 [00:00<00:00,  8.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Two-Tower Top-10 Results:\n",
      "   1.  | Sim: 0.3707 | Toddler Kids Baby Girl Ruffle 3/4 Long Flare Sleeve Floral P...\n",
      "   2.  | Sim: 0.3591 | White and Orange and Medium Yellow Latex Balloons...\n",
      "   3.  | Sim: 0.3539 | 80 Pieces Mardi Gras Balloons Latex Balloons Confetti Balloo...\n",
      "   4.  | Sim: 0.3389 | Weileenice 3-14Y Flower Girl Wedding Dresses 3T 4T Toddler C...\n",
      "   5.  | Sim: 0.3297 | Todaies Women Cardigan Coat, Women Long Sleeve Loose Coat Ov...\n",
      "   6.  | Sim: 0.3254 | AYHome Kids Animal Onesie Unicorn Costume Christmas Hallowee...\n",
      "   7.  | Sim: 0.3254 | Fancy Ivory White Lace Flower Girl Dress Boho Rustic First C...\n",
      "   8.  | Sim: 0.3251 | Whaline 80Pcs Fall Balloon Set Autumn Leave Maple Leave Pump...\n",
      "   9.  | Sim: 0.3214 | Simplee Women's Casual Open Front Long Sleeve Knit Cardigan ...\n",
      "  10.  | Sim: 0.3188 | Tueenhuge Baby Girls Christmas Dress Toddler Snowflake Print...\n",
      "\n",
      " Two-Tower Results:\n",
      "    Precision@10: 0.0000\n",
      "    Recall@10: 0.0000\n",
      "    Found 0 out of 8 ground truth items in Top 10\n",
      "\n",
      " Comparison Summary:\n",
      "    TF-IDF:              Found 6/8 ground truth items (Precision: 0.6000)\n",
      "    SentenceTransformers: Found 7/8 ground truth items (Precision: 0.7000)\n",
      "    Two-Tower:           Found 0/8 ground truth items (Precision: 0.0000)\n",
      "\n",
      " Running comprehensive evaluation on all test queries...\n",
      "Calculating evaluation metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 10.05it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 12.53it/s]t/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 12.95it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 12.76it/s]t/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 13.50it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 13.19it/s]t/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 13.97it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 12.84it/s]t/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 14.12it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 14.27it/s]t/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 14.58it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 13.72it/s]it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 14.08it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 12.83it/s]it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 14.75it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 13.85it/s]it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 14.81it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 13.58it/s]it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 14.10it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 14.84it/s]it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 13.74it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 14.66it/s]it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 14.21it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 14.05it/s]it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 14.43it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 14.06it/s]it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 14.81it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 14.75it/s]it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 14.67it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 14.91it/s]it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 15.23it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 14.78it/s]it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 14.78it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 14.68it/s]it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 14.73it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 15.18it/s]it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 14.78it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 15.82it/s]it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 14.81it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 14.94it/s]it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 14.53it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 15.66it/s]it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 14.48it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 15.86it/s]it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 14.70it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 15.56it/s]it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 15.30it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 14.42it/s]it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 14.67it/s]\n",
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 15.85it/s]it/s]\n",
      "Evaluating queries: 100%|| 50/50 [00:03<00:00, 12.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Comprehensive Results:\n",
      "Metric       Top-1    Top-5    Top-10   Top-20  \n",
      "------------------------------------------------\n",
      "PRECISION   0.020   0.020   0.020   0.021   \n",
      "RECALL      0.002   0.007   0.015   0.038   \n",
      "NDCG        0.000   0.652   0.565   0.449   \n",
      "MRR         0.020   0.034   0.037   0.042   \n",
      "\n",
      " Testing additional sample queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 15.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  '6 dining chairs': 0/22 relevant in Top-5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 17.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  'plants': 0/22 relevant in Top-5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding querys: 100%|| 1/1 [00:00<00:00, 15.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  'turning shoe': 0/30 relevant in Top-5\n",
      "\n",
      " Two-Tower evaluation completed!\n",
      "\n",
      "============================================================\n",
      " FINAL COMPARISON SUMMARY\n",
      "============================================================\n",
      "Method                | Precision@10 | Recall@10 | Notes\n",
      "------------------------------------------------------------\n",
      "TF-IDF               |    0.6000    |   0.7500  | Sparse, keyword-based\n",
      "SentenceTransformers |    0.7000    |   0.8750  | Dense, pre-trained\n",
      "Two-Tower (Custom)   |    ?.????    |   ?.????  | Dense, task-specific\n",
      "\n",
      " The Two-Tower model should perform better due to:\n",
      "   - Task-specific training on your exact data\n",
      "   - Separate encoders for queries and products\n",
      "   - Triplet loss optimization for retrieval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Two-Tower Model Evaluation ===\")\n",
    "\n",
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from two_tower_evaluation import TwoTowerEvaluator\n",
    "\n",
    "# Check if model exists\n",
    "model_path = './two_tower_model'\n",
    "if not os.path.exists(model_path):\n",
    "    print(\" Two-Tower model not found. Please train the model first using two_tower_fixed.py\")\n",
    "else:\n",
    "    print(\" Two-Tower model found. Starting evaluation...\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize evaluator\n",
    "        evaluator = TwoTowerEvaluator(\n",
    "            model_path=model_path,\n",
    "            test_data_path='sample_2c_full_data.csv'\n",
    "        )\n",
    "        \n",
    "        # Run evaluation\n",
    "        print(\"\\n Building product embeddings index...\")\n",
    "        evaluator.build_product_index()\n",
    "        \n",
    "        # Test the same query as baselines for comparison\n",
    "        test_query = \"carpenter bench press\"\n",
    "        print(f\"\\n Testing query: '{test_query}'\")\n",
    "        \n",
    "        # Get ground truth for comparison\n",
    "        ground_truth_df = evaluator.eval_data[evaluator.eval_data['query'] == test_query]\n",
    "        ground_truth_ids = set(ground_truth_df[ground_truth_df['esci_label'] == 'E']['product_id'].tolist())\n",
    "        \n",
    "        print(f\" Ground Truth: {len(ground_truth_ids)} relevant products\")\n",
    "        print(f\"    Product IDs: {list(ground_truth_ids)}\")\n",
    "        \n",
    "        # Search using Two-Tower model\n",
    "        results = evaluator.search_products(test_query, top_k=10)\n",
    "        \n",
    "        print(f\"\\n Two-Tower Top-10 Results:\")\n",
    "        hits = 0\n",
    "        for i, (product_id, similarity) in enumerate(results, 1):\n",
    "            is_relevant = product_id in ground_truth_ids\n",
    "            if is_relevant:\n",
    "                hits += 1\n",
    "            \n",
    "            # Get product title\n",
    "            product_title = evaluator.product_corpus[evaluator.product_corpus['product_id'] == product_id]['product_title'].iloc[0]\n",
    "            status = \" RELEVANT\" if is_relevant else \"\"\n",
    "            \n",
    "            print(f\"  {i:2d}. {status} | Sim: {similarity:.4f} | {product_title[:60]}...\")\n",
    "        \n",
    "        two_tower_precision = hits / 10\n",
    "        two_tower_recall = hits / len(ground_truth_ids) if ground_truth_ids else 0\n",
    "        \n",
    "        print(f\"\\n Two-Tower Results:\")\n",
    "        print(f\"    Precision@10: {two_tower_precision:.4f}\")\n",
    "        print(f\"    Recall@10: {two_tower_recall:.4f}\")\n",
    "        print(f\"    Found {hits} out of {len(ground_truth_ids)} ground truth items in Top 10\")\n",
    "        \n",
    "        # Compare with previous results\n",
    "        print(f\"\\n Comparison Summary:\")\n",
    "        print(f\"    TF-IDF:              Found 6/8 ground truth items (Precision: 0.6000)\")\n",
    "        print(f\"    SentenceTransformers: Found 7/8 ground truth items (Precision: 0.7000)\")\n",
    "        print(f\"    Two-Tower:           Found {hits}/{len(ground_truth_ids)} ground truth items (Precision: {two_tower_precision:.4f})\")\n",
    "        \n",
    "        # Run comprehensive evaluation\n",
    "        print(f\"\\n Running comprehensive evaluation on all test queries...\")\n",
    "        metrics = evaluator.calculate_metrics([1, 5, 10, 20])\n",
    "        avg_metrics, _ = metrics\n",
    "        \n",
    "        print(f\"\\n Comprehensive Results:\")\n",
    "        print(f\"{'Metric':<12} {'Top-1':<8} {'Top-5':<8} {'Top-10':<8} {'Top-20':<8}\")\n",
    "        print(\"-\" * 48)\n",
    "        \n",
    "        for metric_name in ['precision', 'recall', 'ndcg', 'mrr']:\n",
    "            row = f\"{metric_name.upper():<12}\"\n",
    "            for k in [1, 5, 10, 20]:\n",
    "                row += f\"{avg_metrics[k][metric_name]:<8.3f}\"\n",
    "            print(row)\n",
    "        \n",
    "        # Sample additional queries\n",
    "        print(f\"\\n Testing additional sample queries...\")\n",
    "        sample_queries = ['6 dining chairs', 'plants', 'turning shoe']\n",
    "        \n",
    "        for query in sample_queries:\n",
    "            if query in evaluator.eval_data['query'].values:\n",
    "                ground_truth = evaluator.eval_data[evaluator.eval_data['query'] == query]\n",
    "                relevant_count = len(ground_truth[ground_truth['esci_label'] == 'E'])\n",
    "                \n",
    "                results = evaluator.search_products(query, top_k=5)\n",
    "                relevant_in_top5 = sum(1 for pid, _ in results if pid in set(ground_truth[ground_truth['esci_label'] == 'E']['product_id']))\n",
    "                \n",
    "                print(f\"  '{query}': {relevant_in_top5}/{relevant_count} relevant in Top-5\")\n",
    "        \n",
    "        print(f\"\\n Two-Tower evaluation completed!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error during evaluation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" FINAL COMPARISON SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(\"Method                | Precision@10 | Recall@10 | Notes\")\n",
    "print(\"-\" * 60)\n",
    "print(\"TF-IDF               |    0.6000    |   0.7500  | Sparse, keyword-based\")\n",
    "print(\"SentenceTransformers |    0.7000    |   0.8750  | Dense, pre-trained\")\n",
    "print(\"Two-Tower (Custom)   |    ?.????    |   ?.????  | Dense, task-specific\")\n",
    "print(\"\\n The Two-Tower model should perform better due to:\")\n",
    "print(\"   - Task-specific training on your exact data\")\n",
    "print(\"   - Separate encoders for queries and products\")\n",
    "print(\"   - Triplet loss optimization for retrieval\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
